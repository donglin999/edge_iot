#!/usr/bin/env python
# -*- encoding: utf-8 -*-
"""
å†…å­˜æ³„æ¼é™æ€åˆ†æ - connect_melseca1enet_backu
"""

import unittest
import ast
import re
import sys
import os
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from tests.base import BaseTestCase


class MemoryLeakStaticAnalyzer:
    """å†…å­˜æ³„æ¼é™æ€åˆ†æå™¨"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.source_code = None
        self.ast_tree = None
        self.issues = []
        self.suggestions = []
        
    def load_source(self):
        """åŠ è½½æºä»£ç """
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                self.source_code = f.read()
            self.ast_tree = ast.parse(self.source_code)
            return True
        except Exception as e:
            print(f"åŠ è½½æºä»£ç å¤±è´¥: {e}")
            return False
    
    def analyze_connection_management(self):
        """åˆ†æè¿æ¥ç®¡ç†é—®é¢˜"""
        print("ğŸ” åˆ†æè¿æ¥ç®¡ç†...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ææ„å‡½æ•°
        has_destructor = '__del__' in self.source_code
        if not has_destructor:
            self.issues.append({
                "type": "è¿æ¥ç®¡ç†",
                "severity": "é«˜",
                "issue": "MelsecA1ENetClientç±»ç¼ºå°‘__del__ææ„å‡½æ•°",
                "description": "æ²¡æœ‰ææ„å‡½æ•°å¯èƒ½å¯¼è‡´PLCè¿æ¥ã€InfluxDBè¿æ¥å’ŒKafkaè¿æ¥æ— æ³•æ­£ç¡®å…³é—­",
                "line": "ç±»å®šä¹‰å¤„"
            })
            self.suggestions.append("æ·»åŠ __del__æ–¹æ³•ç¡®ä¿æ‰€æœ‰è¿æ¥æ­£ç¡®å…³é—­")
        
        # æ£€æŸ¥è¿æ¥é‡å»ºé€»è¾‘
        reconnect_pattern = r'self\.plc\s*=\s*MelsecA1ENet'
        reconnect_matches = re.findall(reconnect_pattern, self.source_code)
        if len(reconnect_matches) > 1:
            self.issues.append({
                "type": "è¿æ¥ç®¡ç†", 
                "severity": "é«˜",
                "issue": f"å‘ç°{len(reconnect_matches)}å¤„PLCè¿æ¥é‡å»º",
                "description": "åœ¨å¼‚å¸¸å¤„ç†ä¸­é‡å»ºè¿æ¥ä½†æœªå…ˆå…³é—­æ—§è¿æ¥ï¼Œå¯èƒ½é€ æˆè¿æ¥æ³„æ¼",
                "line": "å¼‚å¸¸å¤„ç†ä»£ç å—"
            })
            self.suggestions.append("åœ¨é‡å»ºè¿æ¥å‰å…ˆè°ƒç”¨ConnectClose()å…³é—­æ—§è¿æ¥")
    
    def analyze_string_operations(self):
        """åˆ†æå­—ç¬¦ä¸²æ“ä½œé—®é¢˜"""
        print("ğŸ” åˆ†æå­—ç¬¦ä¸²æ“ä½œ...")
        
        # æ£€æŸ¥å¤§å­—ç¬¦ä¸²æ“ä½œ
        large_string_operations = [
            r'str\(.*\.ReadString\(.*\)\).*\[4:64\]',  # å­—ç¬¦ä¸²åˆ‡ç‰‡
            r're\.sub\(',  # æ­£åˆ™æ›¿æ¢
            r'\.Content\)',  # å¤šæ¬¡Contentè®¿é—®
        ]
        
        for pattern in large_string_operations:
            matches = re.findall(pattern, self.source_code)
            if matches:
                self.issues.append({
                    "type": "å­—ç¬¦ä¸²å¤„ç†",
                    "severity": "ä¸­",
                    "issue": f"å‘ç°{len(matches)}å¤„å¯èƒ½çš„å†…å­˜å¯†é›†å‹å­—ç¬¦ä¸²æ“ä½œ",
                    "description": "å¤§é‡å­—ç¬¦ä¸²å¤„ç†å’Œæ­£åˆ™æ“ä½œå¯èƒ½åˆ›å»ºå¾ˆå¤šä¸´æ—¶å¯¹è±¡",
                    "line": "read_plcæ–¹æ³•ä¸­"
                })
        
        # æ£€æŸ¥å­—ç¬¦ä¸²æ‹¼æ¥
        string_concat_pattern = r'".*"\s*\+\s*.*\+\s*".*"'
        concat_matches = re.findall(string_concat_pattern, self.source_code)
        if concat_matches:
            self.suggestions.append("ä½¿ç”¨f-stringæˆ–joinæ–¹æ³•æ›¿ä»£å­—ç¬¦ä¸²æ‹¼æ¥ä»¥æé«˜æ€§èƒ½")
    
    def analyze_loop_and_iteration(self):
        """åˆ†æå¾ªç¯å’Œè¿­ä»£é—®é¢˜"""
        print("ğŸ” åˆ†æå¾ªç¯å’Œè¿­ä»£...")
        
        # æ£€æŸ¥æ— é™å¾ªç¯
        infinite_loop_pattern = r'while\s+True:'
        infinite_loops = re.findall(infinite_loop_pattern, self.source_code)
        if infinite_loops:
            self.issues.append({
                "type": "å¾ªç¯æ§åˆ¶",
                "severity": "é«˜", 
                "issue": f"å‘ç°{len(infinite_loops)}å¤„æ— é™å¾ªç¯",
                "description": "ä¸»ç¨‹åºå’Œwrite_plcä¸­çš„æ— é™å¾ªç¯å¯èƒ½å¯¼è‡´èµ„æºæ— æ³•é‡Šæ”¾",
                "line": "__main__å’Œwrite_plcæ–¹æ³•"
            })
            self.suggestions.append("ä¸ºæ— é™å¾ªç¯æ·»åŠ é€‚å½“çš„é€€å‡ºæ¡ä»¶å’Œèµ„æºæ¸…ç†é€»è¾‘")
        
        # æ£€æŸ¥å¾ªç¯ä¸­çš„å¯¹è±¡åˆ›å»º
        loop_object_creation = [
            r'for.*in.*:.*Log\(\)',  # å¾ªç¯ä¸­åˆ›å»ºLogå¯¹è±¡
            r'for.*in.*:.*\.append\(',  # å¾ªç¯ä¸­appendæ“ä½œ
        ]
        
        for pattern in loop_object_creation:
            matches = re.findall(pattern, self.source_code, re.DOTALL)
            if matches:
                self.issues.append({
                    "type": "å¾ªç¯ä¼˜åŒ–",
                    "severity": "ä¸­",
                    "issue": "å¾ªç¯ä¸­åˆ›å»ºå¯¹è±¡æˆ–è¿›è¡Œå†…å­˜åˆ†é…",
                    "description": "å¾ªç¯ä¸­é¢‘ç¹åˆ›å»ºå¯¹è±¡å¯èƒ½å¯¼è‡´å†…å­˜ä½¿ç”¨å¢é•¿",
                    "line": "read_plcæ–¹æ³•å¾ªç¯ä¸­"
                })
    
    def analyze_exception_handling(self):
        """åˆ†æå¼‚å¸¸å¤„ç†é—®é¢˜"""
        print("ğŸ” åˆ†æå¼‚å¸¸å¤„ç†...")
        
        # æ£€æŸ¥å¼‚å¸¸å¤„ç†ä¸­çš„èµ„æºç®¡ç†
        exception_blocks = re.findall(r'except.*?:(.*?)(?=\n\s*(?:except|else|finally|\n\S))', 
                                     self.source_code, re.DOTALL)
        
        resource_cleanup_found = False
        for block in exception_blocks:
            if 'ConnectClose' in block or 'close' in block:
                resource_cleanup_found = True
                break
        
        if not resource_cleanup_found:
            self.issues.append({
                "type": "å¼‚å¸¸å¤„ç†",
                "severity": "é«˜",
                "issue": "å¼‚å¸¸å¤„ç†ä¸­ç¼ºå°‘èµ„æºæ¸…ç†",
                "description": "å¼‚å¸¸å‘ç”Ÿæ—¶å¯èƒ½å¯¼è‡´è¿æ¥å’Œèµ„æºæ— æ³•æ­£ç¡®é‡Šæ”¾",
                "line": "æ‰€æœ‰exceptå—"
            })
            self.suggestions.append("åœ¨å¼‚å¸¸å¤„ç†ä¸­æ·»åŠ èµ„æºæ¸…ç†ä»£ç ")
    
    def analyze_data_structures(self):
        """åˆ†ææ•°æ®ç»“æ„ä½¿ç”¨"""
        print("ğŸ” åˆ†ææ•°æ®ç»“æ„ä½¿ç”¨...")
        
        # æ£€æŸ¥åˆ—è¡¨å’Œå­—å…¸çš„ä½¿ç”¨æ¨¡å¼
        if 'tag_data = []' in self.source_code:
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„åˆ—è¡¨å¤§å°æ§åˆ¶
            if 'tag_data.clear()' not in self.source_code and 'del tag_data' not in self.source_code:
                self.issues.append({
                    "type": "æ•°æ®ç»“æ„",
                    "severity": "ä¸­",
                    "issue": "åˆ—è¡¨tag_dataå¯èƒ½æ— é™å¢é•¿",
                    "description": "åœ¨é•¿æ—¶é—´è¿è¡Œä¸­ï¼Œtag_dataåˆ—è¡¨å¯èƒ½ç§¯ç´¯å¤§é‡æ•°æ®è€Œä¸æ¸…ç†",
                    "line": "read_plcæ–¹æ³•"
                })
                self.suggestions.append("å®šæœŸæ¸…ç†æˆ–é™åˆ¶tag_dataåˆ—è¡¨å¤§å°")
        
        # æ£€æŸ¥å­—å…¸åˆ›å»ºæ¨¡å¼
        dict_creation_pattern = r'plc_data\s*=\s*{}'
        dict_matches = re.findall(dict_creation_pattern, self.source_code)
        if len(dict_matches) > 5:
            self.suggestions.append("è€ƒè™‘é‡ç”¨å­—å…¸å¯¹è±¡è€Œä¸æ˜¯æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„")
    
    def analyze_third_party_integrations(self):
        """åˆ†æç¬¬ä¸‰æ–¹é›†æˆé—®é¢˜"""
        print("ğŸ” åˆ†æç¬¬ä¸‰æ–¹é›†æˆ...")
        
        # æ£€æŸ¥Kafkaé›†æˆ
        if 'KafkaProducer' in self.source_code:
            if 'kafka_producer.close()' not in self.source_code:
                self.issues.append({
                    "type": "ç¬¬ä¸‰æ–¹é›†æˆ",
                    "severity": "ä¸­",
                    "issue": "Kafka Produceræœªæ˜¾å¼å…³é—­",
                    "description": "KafkaProducerå¯¹è±¡å¯èƒ½åœ¨ç¨‹åºç»“æŸæ—¶æœªæ­£ç¡®å…³é—­",
                    "line": "__init__æ–¹æ³•"
                })
                self.suggestions.append("åœ¨ææ„å‡½æ•°ä¸­å…³é—­KafkaProducer")
        
        # æ£€æŸ¥InfluxDBé›†æˆ  
        if 'InfluxClient' in self.source_code:
            if 'influxdb_client.close()' not in self.source_code:
                self.issues.append({
                    "type": "ç¬¬ä¸‰æ–¹é›†æˆ", 
                    "severity": "ä¸­",
                    "issue": "InfluxDBå®¢æˆ·ç«¯æœªæ˜¾å¼å…³é—­",
                    "description": "InfluxDBè¿æ¥å¯èƒ½åœ¨ç¨‹åºç»“æŸæ—¶æœªæ­£ç¡®å…³é—­",
                    "line": "__init__æ–¹æ³•"
                })
                self.suggestions.append("åœ¨ææ„å‡½æ•°ä¸­å…³é—­InfluxDBè¿æ¥")
    
    def analyze_memory_patterns(self):
        """åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼"""
        print("ğŸ” åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼...")
        
        # æ£€æŸ¥åƒåœ¾å›æ”¶
        if 'gc.collect()' not in self.source_code:
            self.suggestions.append("åœ¨é€‚å½“ä½ç½®æ·»åŠ gc.collect()è¿›è¡Œåƒåœ¾å›æ”¶")
        
        # æ£€æŸ¥å¤§å¯¹è±¡å¤„ç†
        large_object_patterns = [
            r'struct\.pack',
            r'struct\.unpack', 
            r'ReadString.*,\s*\d{2,}',  # è¯»å–å¤§é‡å­—ç¬¦ä¸²
        ]
        
        for pattern in large_object_patterns:
            matches = re.findall(pattern, self.source_code)
            if matches:
                self.issues.append({
                    "type": "å†…å­˜æ¨¡å¼",
                    "severity": "ä¸­",
                    "issue": f"å‘ç°{len(matches)}å¤„å¤§å¯¹è±¡æ“ä½œ",
                    "description": "å¤§å¯¹è±¡æ“ä½œå¯èƒ½å¯¼è‡´å†…å­˜å³°å€¼ä½¿ç”¨",
                    "line": "æ•°æ®å¤„ç†éƒ¨åˆ†"
                })
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ å†…å­˜æ³„æ¼é™æ€åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        if not self.issues:
            print("âœ… æœªå‘ç°æ˜æ˜¾çš„å†…å­˜æ³„æ¼é£é™©")
            return
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        high_issues = [i for i in self.issues if i['severity'] == 'é«˜']
        medium_issues = [i for i in self.issues if i['severity'] == 'ä¸­']
        low_issues = [i for i in self.issues if i['severity'] == 'ä½']
        
        print(f"\nğŸš¨ é«˜é£é™©é—®é¢˜ ({len(high_issues)}ä¸ª):")
        for i, issue in enumerate(high_issues, 1):
            print(f"  {i}. [{issue['type']}] {issue['issue']}")
            print(f"     ä½ç½®: {issue['line']}")
            print(f"     è¯´æ˜: {issue['description']}")
            print()
        
        print(f"\nâš ï¸  ä¸­é£é™©é—®é¢˜ ({len(medium_issues)}ä¸ª):")
        for i, issue in enumerate(medium_issues, 1):
            print(f"  {i}. [{issue['type']}] {issue['issue']}")
            print(f"     ä½ç½®: {issue['line']}")
            print(f"     è¯´æ˜: {issue['description']}")
            print()
        
        if low_issues:
            print(f"\nğŸ’¡ ä½é£é™©é—®é¢˜ ({len(low_issues)}ä¸ª):")
            for i, issue in enumerate(low_issues, 1):
                print(f"  {i}. [{issue['type']}] {issue['issue']}")
                print(f"     è¯´æ˜: {issue['description']}")
                print()
        
        print(f"\nğŸ”§ ä¿®å¤å»ºè®®:")
        for i, suggestion in enumerate(self.suggestions, 1):
            print(f"  {i}. {suggestion}")
        
        return {
            'high_issues': len(high_issues),
            'medium_issues': len(medium_issues), 
            'low_issues': len(low_issues),
            'total_issues': len(self.issues),
            'suggestions': len(self.suggestions)
        }
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        if not self.load_source():
            return None
        
        print(f"ğŸ“ åˆ†ææ–‡ä»¶: {self.file_path}")
        print(f"ğŸ“Š æºä»£ç è¡Œæ•°: {len(self.source_code.splitlines())}")
        
        # è¿è¡Œå„é¡¹åˆ†æ
        self.analyze_connection_management()
        self.analyze_string_operations()
        self.analyze_loop_and_iteration() 
        self.analyze_exception_handling()
        self.analyze_data_structures()
        self.analyze_third_party_integrations()
        self.analyze_memory_patterns()
        
        return self.generate_report()


class TestMelsecA1ENetStaticAnalysis(BaseTestCase):
    """MelsecA1ENet é™æ€åˆ†ææµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        super().setUp()
        self.target_file = str(PROJECT_ROOT / "apps" / "connect" / "connect_melseca1enet_backu.py")
        self.analyzer = MemoryLeakStaticAnalyzer(self.target_file)
    
    def test_static_memory_leak_analysis(self):
        """é™æ€å†…å­˜æ³„æ¼åˆ†æ"""
        print("\nğŸš€ å¼€å§‹é™æ€å†…å­˜æ³„æ¼åˆ†æ...")
        
        # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å­˜åœ¨
        self.assertTrue(os.path.exists(self.target_file), f"ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {self.target_file}")
        
        # è¿è¡Œåˆ†æ
        result = self.analyzer.run_full_analysis()
        
        # éªŒè¯åˆ†æç»“æœ
        self.assertIsNotNone(result, "åˆ†æåº”è¯¥è¿”å›ç»“æœ")
        self.assertIsInstance(result, dict, "åˆ†æç»“æœåº”è¯¥æ˜¯å­—å…¸æ ¼å¼")
        
        # æ£€æŸ¥æ˜¯å¦å‘ç°äº†é¢„æœŸçš„é—®é¢˜
        self.assertGreater(result['total_issues'], 0, "åº”è¯¥å‘ç°ä¸€äº›æ½œåœ¨é—®é¢˜")
        self.assertGreater(result['suggestions'], 0, "åº”è¯¥æä¾›ä¿®å¤å»ºè®®")
        
        # å¦‚æœå‘ç°é«˜é£é™©é—®é¢˜ï¼Œæµ‹è¯•åº”è¯¥ç»™å‡ºè­¦å‘Š
        if result['high_issues'] > 0:
            print(f"\nâš ï¸  å‘ç° {result['high_issues']} ä¸ªé«˜é£é™©å†…å­˜æ³„æ¼é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤ï¼")
        
        if result['medium_issues'] > 0:
            print(f"\nğŸ’¡ å‘ç° {result['medium_issues']} ä¸ªä¸­é£é™©é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–")
        
        # è¿”å›åˆ†æç»“æœä¾›åç»­ä½¿ç”¨
        return result
    
    def test_generate_fix_recommendations(self):
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        print("\nğŸ”¨ ç”Ÿæˆä¿®å¤å»ºè®®...")
        
        # è¿è¡Œåˆ†æ
        self.analyzer.run_full_analysis()
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå…·ä½“çš„ä¿®å¤ä»£ç å»ºè®®
        fix_recommendations = self.generate_fix_code()
        
        print("\nğŸ“ å…·ä½“ä¿®å¤ä»£ç å»ºè®®:")
        for i, (title, code) in enumerate(fix_recommendations.items(), 1):
            print(f"\n{i}. {title}:")
            print("-" * 40)
            print(code)
    
    def generate_fix_code(self):
        """ç”Ÿæˆå…·ä½“çš„ä¿®å¤ä»£ç """
        recommendations = {}
        
        # 1. æ·»åŠ ææ„å‡½æ•°
        recommendations["æ·»åŠ ææ„å‡½æ•°ç¡®ä¿èµ„æºé‡Šæ”¾"] = '''
def __del__(self):
    """ææ„å‡½æ•° - ç¡®ä¿æ‰€æœ‰è¿æ¥æ­£ç¡®å…³é—­"""
    try:
        # å…³é—­PLCè¿æ¥
        if hasattr(self, 'plc') and self.plc:
            self.plc.ConnectClose()
            
        # å…³é—­InfluxDBè¿æ¥
        if hasattr(self, 'influxdb_client') and self.influxdb_client:
            self.influxdb_client.close()
            
        # å…³é—­Kafkaè¿æ¥
        if hasattr(self, 'kafka_producer') and self.kafka_producer:
            self.kafka_producer.close()
            
        print("MelsecA1ENetClient èµ„æºå·²é‡Šæ”¾")
    except Exception as e:
        print(f"èµ„æºé‡Šæ”¾æ—¶å‡ºç°å¼‚å¸¸: {e}")'''
        
        # 2. ä¼˜åŒ–å¼‚å¸¸å¤„ç†
        recommendations["ä¼˜åŒ–å¼‚å¸¸å¤„ç†ä¸­çš„è¿æ¥é‡å»º"] = '''
def reconnect_plc(self):
    """å®‰å…¨åœ°é‡æ–°è¿æ¥PLC"""
    try:
        # å…ˆå…³é—­æ—§è¿æ¥
        if hasattr(self, 'plc') and self.plc:
            self.plc.ConnectClose()
            
        # åˆ›å»ºæ–°è¿æ¥
        self.plc = MelsecA1ENet(self.ip, self.port)
        result = self.plc.ConnectServer()
        
        if result.IsSuccess:
            Log().printInfo(f"PLCé‡è¿æˆåŠŸ: {self.ip}:{self.port}")
            return True
        else:
            Log().printError(f"PLCé‡è¿å¤±è´¥: {result.Message}")
            return False
            
    except Exception as e:
        Log().printError(f"PLCé‡è¿å¼‚å¸¸: {e}")
        return False'''
        
        # 3. ä¼˜åŒ–read_plcæ–¹æ³•
        recommendations["ä¼˜åŒ–read_plcæ–¹æ³•å‡å°‘å†…å­˜åˆ†é…"] = '''
def read_plc(self, register_dict):
    """ä¼˜åŒ–åçš„PLCè¯»å–æ–¹æ³•"""
    tag_data = []
    
    try:
        # é¢„åˆ†é…å­—å…¸ï¼Œå‡å°‘é‡å¤åˆ›å»º
        base_plc_data = {
            'kafka_position': '',
            'cn_name': '',
            'device_a_tag': '',
            'device_name': ''
        }
        
        for en_name, register_conf in register_dict.items():
            if not isinstance(register_conf, dict):
                continue
                
            try:
                # å¤ç”¨åŸºç¡€å­—å…¸ç»“æ„
                plc_data = base_plc_data.copy()
                plc_data.update({
                    'kafka_position': register_conf['kafka_position'],
                    'cn_name': register_conf['cn_name'],
                    'device_a_tag': register_conf['device_a_tag'],
                    'device_name': register_conf['device_name']
                })
                
                # è¯»å–æ•°æ®é€»è¾‘...
                value = self._read_by_type(register_conf)
                plc_data[en_name] = value
                
                tag_data.append(plc_data)
                
            except Exception as e:
                Log().printError(f"è¯»å–åœ°å€ {register_conf.get('source_addr', 'unknown')} å¼‚å¸¸: {e}")
                continue
        
        return tag_data
        
    except Exception as e:
        # ä½¿ç”¨å®‰å…¨é‡è¿æ–¹æ³•
        if self.reconnect_plc():
            Log().printInfo("PLCé‡è¿æˆåŠŸï¼Œå°†åœ¨ä¸‹æ¬¡è°ƒç”¨æ—¶é‡è¯•")
        return []
    
    finally:
        # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆåœ¨å¿…è¦æ—¶ï¼‰
        if len(tag_data) > 100:  # åªåœ¨æ•°æ®é‡å¤§æ—¶æ‰è°ƒç”¨
            import gc
            gc.collect()'''
        
        # 4. æ·»åŠ å†…å­˜ç›‘æ§
        recommendations["æ·»åŠ å†…å­˜ç›‘æ§å’Œæ¸…ç†æœºåˆ¶"] = '''
import psutil
import gc

class MemoryMonitor:
    def __init__(self, threshold_mb=100):
        self.threshold_mb = threshold_mb
        self.process = psutil.Process()
        self.read_count = 0
        
    def check_memory(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_mb = self.process.memory_info().rss / 1024 / 1024
        
        if memory_mb > self.threshold_mb:
            Log().printWarning(f"å†…å­˜ä½¿ç”¨è¾ƒé«˜: {memory_mb:.2f}MB")
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
        return memory_mb
    
    def periodic_cleanup(self):
        """å®šæœŸæ¸…ç†"""
        self.read_count += 1
        if self.read_count % 100 == 0:  # æ¯100æ¬¡è¯»å–åæ¸…ç†
            self.check_memory()

# åœ¨MelsecA1ENetClientä¸­æ·»åŠ ï¼š
def __init__(self, ...):
    # ... åŸæœ‰åˆå§‹åŒ–ä»£ç  ...
    self.memory_monitor = MemoryMonitor(threshold_mb=150)'''
        
        # 5. ä¼˜åŒ–å­—ç¬¦ä¸²å¤„ç†
        recommendations["ä¼˜åŒ–å­—ç¬¦ä¸²å¤„ç†å‡å°‘ä¸´æ—¶å¯¹è±¡"] = '''
def process_string_data(self, addr, num, en_name):
    """ä¼˜åŒ–çš„å­—ç¬¦ä¸²å¤„ç†æ–¹æ³•"""
    try:
        # ä½¿ç”¨ä¸€æ¬¡æ€§è¯»å–ï¼Œé¿å…å¤šæ¬¡è®¿é—®Content
        read_result = self.plc.ReadString(addr, num)
        if not read_result.IsSuccess:
            return "ReadError"
            
        raw_content = read_result.Content
        
        if en_name == "codeResult":
            # ä¼˜åŒ–å­—ç¬¦ä¸²åˆ‡ç‰‡å’Œå¤„ç†
            if len(raw_content) >= 64:
                processed = raw_content[4:64]
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„å­—ç¬¦æ£€æŸ¥
                if any(c.isalnum() for c in processed):
                    # ä½¿ç”¨translateæ–¹æ³•æ›¿ä»£æ­£åˆ™è¡¨è¾¾å¼
                    return processed.translate(str.maketrans('', '', ' \\t\\n\\r'))
                else:
                    return "None"
            else:
                return "None"
        else:
            return str(raw_content)
            
    except Exception as e:
        Log().printError(f"å­—ç¬¦ä¸²å¤„ç†å¼‚å¸¸: {e}")
        return "ProcessError"'''
        
        return recommendations


if __name__ == '__main__':
    # è¿è¡Œé™æ€åˆ†ææµ‹è¯•
    unittest.main(verbosity=2)